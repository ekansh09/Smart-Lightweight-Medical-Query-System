{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/megathon/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf6920d03e4a67acad08f2f1320319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiroop.talasila/miniconda3/envs/meg/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model_id = 'medalpaca/medalpaca-7b'\n",
    "\n",
    "device = \"cpu\" #f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = \"<TOKEN>\"\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 29871, 13, 29950, 7889, 29901], [2, 29871, 13, 28956, 13]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    2, 29871,    13, 29950,  7889, 29901]),\n",
       " tensor([    2, 29871,    13, 28956,    13])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.2,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhiroop.talasila/miniconda3/envs/meg/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/abhiroop.talasila/miniconda3/envs/meg/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are tips for managing my bipolar disorder?. Bipolar disorder is a mental health condition that causes extreme mood swings.\n",
      "Bipolar Disorder: Tips for Managing Your Moods. Bipolar disorder can be difficult to manage, but there are steps you can take to help control your symptoms and improve your quality of life.\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"What are tips for managing my bipolar disorder?.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What should I do if I want to stop dialysis?\n",
      " Unterscheidung between hemodialysis and peritoneal dialysis. Hemodialysis uses a machine to filter the blood outside of the body, while peritoneal dialysis uses the lining of the abdominal cavity as a filter. Both methods can be effective for treating kidney failure, but they have different advantages and disadvantages.\n",
      "What are the benefits of stopping dialysis? Stopping dialysis can provide relief from the physical and emotional burdens of treatment, including: 1. Improved quality of life: Dialysis can be a significant burden on a person's daily activities, causing fatigue, pain, and limited mobility. By stopping dialysis, a person may experience improved overall health and well-being. 2. Reduced medication use: Many people on dialysis require multiple medications to control symptoms such as high blood pressure, anemia, and infection. By stopping dialysis, these medications may no longer be necessary, which can reduce the risk of side effects and improve overall health. 3. Increased independence: After stopping dialysis, a person may feel more independent and able to perform daily tasks without relying on medical equipment or assistance. 4. Better sleep: Dialysis can cause fatigue and disrupt sleep patterns, leading to poor sleep quality. By stopping dialysis, a person may experience better sleep and overall energy levels. 5. Improved mental health: The emotional burden of dialysis can take a toll on a person's mental health, leading to depression, anxiety, and other mental health issues. By stopping dialysis, a person may experience improved mental well-being and reduced stress. 6. More time with loved ones: Dialysis can limit a person's ability to spend time with family and friends due to the need for frequent treatments. By stopping dialysis, a person may have more time to spend with loved ones and engage in activities they enjoy. 7. Reduced financial burden: Dialysis can be expensive, and the cost of treatment can place a significant burden on a person's finances. By stopping dialysis, a person may save money on medical expenses and other costs associated with treatment. 8. Greater sense of accomplishment: Stopping dialysis can provide a sense of accomplish\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"What should I do if I want to stop dialysis?\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# checking again that everything is working fine\n",
    "# llm(prompt=\"What are tips for managing my bipolar disorder?.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"] \n",
    "\n",
    "# loader = WebBaseLoader(web_links)\n",
    "# documents = loader.load()\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"train_webmd_squad_v2_consec.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "# storing embeddings in the vector store\n",
    "vectorstore = FAISS.from_documents(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stopping dialysis is a complex decision that requires careful consideration and close monitoring with your healthcare team. While it may be possible to stop dialysis in some cases, it's essential to understand the potential risks and consequences involved. Here are some factors to consider:\n",
      "\n",
      "1. Medical history: If you have a history of heart disease, lung disease, or other serious health conditions, stopping dialysis may not be advisable. Your healthcare team will assess your overall health and determine whether it's safe to stop dialysis.\n",
      "2. Kidney function: If your kidneys are still functioning somewhat, your healthcare team may recommend continuing dialysis to help preserve what little kidney function you have left. However, if your kidneys are no longer functioning, stopping dialysis may not significantly impact your quality of life.\n",
      "3. Personal preferences: It's crucial to consider your personal preferences and values when deciding whether to continue or stop dialysis. Some people may prefer to prioritize their quality of life, while others may be more concerned about the financial burden of continued dialysis treatments.\n",
      "4. Alternative treatment options: Depending on your individual situation, your healthcare team may recommend alternative treatment options, such as a kidney transplant or hemodialysis at home. These options can provide better quality of life and more flexibility in managing your treatment schedule.\n",
      "5. Monitoring and support: Regardless of your decision, it's essential to have regular monitoring and support from your healthcare team. They can help you manage any complications that may arise and ensure your safety and well-being.\n",
      "\n",
      "In summary, stopping dialysis is a complex decision that requires careful consideration and close monitoring with your healthcare team. It's important to discuss your individual circumstances, medical history, and personal preferences with your healthcare provider to determine the best course of action for your specific situation.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What should I do if I want to stop dialysis?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cleantext\n",
    "\n",
    "def generate_context_docs(json_path, output_path=\"webmd_context_docs.txt\"):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    with open(output_path, \"a\") as f:\n",
    "        for x in range(len(data[\"data\"])):\n",
    "            inp = data[\"data\"][x][\"paragraphs\"][0][\"context\"]\n",
    "            inp = cleantext.clean(inp, clean_all=False, extra_spaces=True, stemming=False, stopwords=False,\n",
    "                lowercase=False, numbers=False, punct=False)\n",
    "            \n",
    "            # remove some non info lines\n",
    "            if \"var s_context\" in inp:\n",
    "                continue\n",
    "            f.write(inp)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "generate_context_docs(\"/home/abhiroop.talasila/megathon/data 2/train_webmd_squad_v2_full.json\")\n",
    "generate_context_docs(\"/home/abhiroop.talasila/megathon/data 2/val_webmd_squad_v2_consec.json\")\n",
    "generate_context_docs(\"/home/abhiroop.talasila/megathon/data 2/val_webmd_squad_v2_full.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
